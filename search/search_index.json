{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"x0r notes \u00b6 Hi. This is my attempt to publish my notes onto Github Pages. Hello World! \u00b6 The index.md in the /docs folder is the homepage you see here. The folders in /docs appear as the main sections on the navigation bar.","title":"x0r notes"},{"location":"#x0r-notes","text":"Hi. This is my attempt to publish my notes onto Github Pages.","title":"x0r notes"},{"location":"#hello-world","text":"The index.md in the /docs folder is the homepage you see here. The folders in /docs appear as the main sections on the navigation bar.","title":"Hello World!"},{"location":"About/","text":"About \u00b6 This page is for the fun stuff :)","title":"About"},{"location":"About/#about","text":"This page is for the fun stuff :)","title":"About"},{"location":"cmds/","text":"x0rchive \u00b6 command time date description sudo snap install kontena-lens --classic 17:52:14 07/09/22 visualise your k8s cluster kubectl run nginx --image=nginx 20:33:41 07/09/22 create pod k get pods -o wide 20:35:53 07/09/22 wide info k run redis --image=redis --dry-run=client -o yaml > redis.yaml 20:40:28 07/09/22 creates manifest file k create deployment httpd-frontend --image=httpd:2.4-alpine --replicas=3 --dry-run=client -o yaml > deployment.yaml 22:05:19 07/09/22 no desc provided k run redis --image=redis -l tier=db 22:16:17 07/09/22 add label / imperative cmd kubectl expose pod redis --port=6379 --name redis-service 22:20:57 07/09/22 expose service k run pod custom-nginx --image=nginx --port=8080 22:24:05 07/09/22 exposing container port 8080 docker buildx build --build-context init=init/ . --build-context iwshop=../src/iwshop.repo/ -f iwshop.repo/Dockerfile -t test:v1.0 19:22:27 07/10/22 build docker images w/ multiple build contexts tf init -backend-config=\"access_key=REDACTED\" -backend-config=\"secret_key=REDACTED\" -backend-config=\"region=us-east-1\" 09:24:59 07/20/22 get tf state k exec deploy/iwapi -- printenv 13:41:52 07/21/22 prints env vars from a pod k set image po redis redis=redis 11:57:58 07/24/22 update pod image xargs -a a.txt -n1 -I{} sh -c 'echo {} | base64 -d' 17:10:02 07/24/22 will base64 decode every line from a txt file find . -type f | grep '.json' 17:58:24 07/24/22 lists only .json files find . -type f | grep '.js$' 17:59:19 07/24/22 lists only .js files docker image inspect | gron | grep PATH 18:11:27 07/24/22 the day I met GRON docker image inspect | gron | grep PATH | gron -u 18:11:44 07/24/22 gron reverse","title":"x0rchive"},{"location":"cmds/#x0rchive","text":"command time date description sudo snap install kontena-lens --classic 17:52:14 07/09/22 visualise your k8s cluster kubectl run nginx --image=nginx 20:33:41 07/09/22 create pod k get pods -o wide 20:35:53 07/09/22 wide info k run redis --image=redis --dry-run=client -o yaml > redis.yaml 20:40:28 07/09/22 creates manifest file k create deployment httpd-frontend --image=httpd:2.4-alpine --replicas=3 --dry-run=client -o yaml > deployment.yaml 22:05:19 07/09/22 no desc provided k run redis --image=redis -l tier=db 22:16:17 07/09/22 add label / imperative cmd kubectl expose pod redis --port=6379 --name redis-service 22:20:57 07/09/22 expose service k run pod custom-nginx --image=nginx --port=8080 22:24:05 07/09/22 exposing container port 8080 docker buildx build --build-context init=init/ . --build-context iwshop=../src/iwshop.repo/ -f iwshop.repo/Dockerfile -t test:v1.0 19:22:27 07/10/22 build docker images w/ multiple build contexts tf init -backend-config=\"access_key=REDACTED\" -backend-config=\"secret_key=REDACTED\" -backend-config=\"region=us-east-1\" 09:24:59 07/20/22 get tf state k exec deploy/iwapi -- printenv 13:41:52 07/21/22 prints env vars from a pod k set image po redis redis=redis 11:57:58 07/24/22 update pod image xargs -a a.txt -n1 -I{} sh -c 'echo {} | base64 -d' 17:10:02 07/24/22 will base64 decode every line from a txt file find . -type f | grep '.json' 17:58:24 07/24/22 lists only .json files find . -type f | grep '.js$' 17:59:19 07/24/22 lists only .js files docker image inspect | gron | grep PATH 18:11:27 07/24/22 the day I met GRON docker image inspect | gron | grep PATH | gron -u 18:11:44 07/24/22 gron reverse","title":"x0rchive"},{"location":"Docker/Note/","text":"Installing Docker on Kali Linux \u00b6 docker-ce can be installed from Docker repository. One thing to bear in mind, Kali Linux is based on Debian , so we need to use Debian\u2019s current stable version (even though Kali Linux is a rolling distribution ). At the time of writing (Dec. 2021), its \u201cbullseye\u201d: printf '%s\\n' \"deb https://download.docker.com/linux/debian bullseye stable\" | sudo tee /etc/apt/sources.list.d/docker-ce.list curl -fsSL https://download.docker.com/linux/debian/gpg | sudo gpg --dearmor -o /etc/apt/trusted.gpg.d/docker-ce-archive-keyring.gpg sudo apt update sudo apt install -y docker-ce docker-ce-cli containerd.io","title":"Installing Docker on Kali Linux"},{"location":"Docker/Note/#installing-docker-on-kali-linux","text":"docker-ce can be installed from Docker repository. One thing to bear in mind, Kali Linux is based on Debian , so we need to use Debian\u2019s current stable version (even though Kali Linux is a rolling distribution ). At the time of writing (Dec. 2021), its \u201cbullseye\u201d: printf '%s\\n' \"deb https://download.docker.com/linux/debian bullseye stable\" | sudo tee /etc/apt/sources.list.d/docker-ce.list curl -fsSL https://download.docker.com/linux/debian/gpg | sudo gpg --dearmor -o /etc/apt/trusted.gpg.d/docker-ce-archive-keyring.gpg sudo apt update sudo apt install -y docker-ce docker-ce-cli containerd.io","title":"Installing Docker on Kali Linux"},{"location":"Kubernetes/01%20Core%20Concepts/00%20-%20Pods/","text":"PODs \u00b6 list pods k get pods create a new pod with nginx image k run nginx --image = nginx # k run <name of pod> --image=<your image> check the image of a pod k describe pod <pod name> check on which nodes are the pods placed k get pods -o wide The READY column in the output of the k get pods command indicates Running containers in POD/Total containers in POD delete a pod k delete pod <pod name> once created, we can edit the pod configuration with k edit pod <pod name>","title":"PODs"},{"location":"Kubernetes/01%20Core%20Concepts/00%20-%20Pods/#pods","text":"list pods k get pods create a new pod with nginx image k run nginx --image = nginx # k run <name of pod> --image=<your image> check the image of a pod k describe pod <pod name> check on which nodes are the pods placed k get pods -o wide The READY column in the output of the k get pods command indicates Running containers in POD/Total containers in POD delete a pod k delete pod <pod name> once created, we can edit the pod configuration with k edit pod <pod name>","title":"PODs"},{"location":"Kubernetes/01%20Core%20Concepts/05%20-%20ReplicaSets/","text":"ReplicaSets \u00b6 ReplicaSets ensures that the desired number of PODs always run how many replicasets exist on the system? k get replicasets ReplicaSet example apiVersion : apps/v1 kind : ReplicaSet metadata : name : replicaset-1 spec : replicas : 2 selector : matchLabels : tier : frontend template : metadata : labels : tier : frontend spec : containers : - name : nginx image : nginx we can edit the replicaset config by k edit replicaset <replicaset name> If we already have running pods from the replicaSet the changes will not take effect unless we delete(recreate) the pods. we can scale replicasets by editing the manifest or over terminal with: k scale replicasets <name of the replicaSet> --replicas = <NUMBER OF REPLICAS>","title":"ReplicaSets"},{"location":"Kubernetes/01%20Core%20Concepts/05%20-%20ReplicaSets/#replicasets","text":"ReplicaSets ensures that the desired number of PODs always run how many replicasets exist on the system? k get replicasets ReplicaSet example apiVersion : apps/v1 kind : ReplicaSet metadata : name : replicaset-1 spec : replicas : 2 selector : matchLabels : tier : frontend template : metadata : labels : tier : frontend spec : containers : - name : nginx image : nginx we can edit the replicaset config by k edit replicaset <replicaset name> If we already have running pods from the replicaSet the changes will not take effect unless we delete(recreate) the pods. we can scale replicasets by editing the manifest or over terminal with: k scale replicasets <name of the replicaSet> --replicas = <NUMBER OF REPLICAS>","title":"ReplicaSets"},{"location":"Kubernetes/01%20Core%20Concepts/10%20-%20Deployments/","text":"Deployments \u00b6 deployment example --- apiVersion : apps/v1 kind : Deployment metadata : name : deployment-1 spec : replicas : 2 selector : matchLabels : name : busybox-pod template : metadata : labels : name : busybox-pod spec : containers : - name : busybox-container image : busybox888 command : - sh - \"-c\" - echo Hello Kubernetes! && sleep 3600 creating deployment with kubectl k create deployment httpd-frontend --image=httpd:2.4-alpine --replicas=3 To preview the object that would be applied to the cluster without really submitting it: k create deployment httpd-frontend --image = httpd:2.4-alpine --replicas = 3 --dry-run = client -o yaml > deployment.yaml","title":"Deployments"},{"location":"Kubernetes/01%20Core%20Concepts/10%20-%20Deployments/#deployments","text":"deployment example --- apiVersion : apps/v1 kind : Deployment metadata : name : deployment-1 spec : replicas : 2 selector : matchLabels : name : busybox-pod template : metadata : labels : name : busybox-pod spec : containers : - name : busybox-container image : busybox888 command : - sh - \"-c\" - echo Hello Kubernetes! && sleep 3600 creating deployment with kubectl k create deployment httpd-frontend --image=httpd:2.4-alpine --replicas=3 To preview the object that would be applied to the cluster without really submitting it: k create deployment httpd-frontend --image = httpd:2.4-alpine --replicas = 3 --dry-run = client -o yaml > deployment.yaml","title":"Deployments"},{"location":"Kubernetes/01%20Core%20Concepts/15%20-%20Namespaces/","text":"Namespaces \u00b6 how to count the number of namespaces in a cluster k get namespaces --noheaders | wc -l create a pod in a specified namespace k run redis --image = redis -n <namespace> find on which namespace your pod is running k get pods --all-namespaces | grep <YOUR POD NAME>","title":"Namespaces"},{"location":"Kubernetes/01%20Core%20Concepts/15%20-%20Namespaces/#namespaces","text":"how to count the number of namespaces in a cluster k get namespaces --noheaders | wc -l create a pod in a specified namespace k run redis --image = redis -n <namespace> find on which namespace your pod is running k get pods --all-namespaces | grep <YOUR POD NAME>","title":"Namespaces"},{"location":"Kubernetes/01%20Core%20Concepts/20%20-%20Services/","text":"Services \u00b6 https://medium.com/swlh/kubernetes-services-simply-visually-explained-2d84e58d70e5 :)","title":"Services"},{"location":"Kubernetes/01%20Core%20Concepts/20%20-%20Services/#services","text":"https://medium.com/swlh/kubernetes-services-simply-visually-explained-2d84e58d70e5 :)","title":"Services"},{"location":"Kubernetes/02%20Scheduling/00%20-%20Manual%20Scheduling/","text":"Manual Scheduling \u00b6 https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/ example how to assign pod to a node(01) yaml apiVersion: v1 kind: Pod metadata: name: nginx spec: containers: - image: nginx name: nginx nodeName: node01","title":"Manual Scheduling"},{"location":"Kubernetes/02%20Scheduling/00%20-%20Manual%20Scheduling/#manual-scheduling","text":"https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/ example how to assign pod to a node(01) yaml apiVersion: v1 kind: Pod metadata: name: nginx spec: containers: - image: nginx name: nginx nodeName: node01","title":"Manual Scheduling"},{"location":"Kubernetes/02%20Scheduling/05%20-%20Labels%20and%20Selectors/","text":"Labels and Selectors \u00b6 example 1: We have deployed a number of PODs. They are labelled with tier , env and bu . How many PODs exist in the dev environment ( env )? example 2: How many objects are in the prod environment including PODs, ReplicaSets and any other objects? example 3: Identify the POD which is part of the prod environment, the finance BU and of frontend tier?","title":"Labels and Selectors"},{"location":"Kubernetes/02%20Scheduling/05%20-%20Labels%20and%20Selectors/#labels-and-selectors","text":"example 1: We have deployed a number of PODs. They are labelled with tier , env and bu . How many PODs exist in the dev environment ( env )? example 2: How many objects are in the prod environment including PODs, ReplicaSets and any other objects? example 3: Identify the POD which is part of the prod environment, the finance BU and of frontend tier?","title":"Labels and Selectors"},{"location":"Kubernetes/02%20Scheduling/10%20-%20Taints%20and%20Tolerations/","text":"Taints & Tolerations \u00b6 check if there are any taints on a node k describe node <NODE NAME> | grep Taints create a taint on node01 with key of spray , value of mortein and effect of NoSchedule : k taint nodes node01 spray = mortein:NoSchedule create a pod named bee with the nginx image, which has a toleration set to the taint mortein . apiVersion : v1 kind : Pod metadata : creationTimestamp : null labels : run : bee name : bee spec : containers : - image : nginx name : bee resources : {} tolerations : - key : spray value : mortein effect : NoSchedule operator : Equal dnsPolicy : ClusterFirst restartPolicy : Always status : {} bee pod was scheduled on node node01 despite the taint. taint/untaint https://medium.com/kubernetes-tutorials/making-sense-of-taints-and-tolerations-in-kubernetes-446e75010f4e","title":"Taints & Tolerations"},{"location":"Kubernetes/02%20Scheduling/10%20-%20Taints%20and%20Tolerations/#taints-tolerations","text":"check if there are any taints on a node k describe node <NODE NAME> | grep Taints create a taint on node01 with key of spray , value of mortein and effect of NoSchedule : k taint nodes node01 spray = mortein:NoSchedule create a pod named bee with the nginx image, which has a toleration set to the taint mortein . apiVersion : v1 kind : Pod metadata : creationTimestamp : null labels : run : bee name : bee spec : containers : - image : nginx name : bee resources : {} tolerations : - key : spray value : mortein effect : NoSchedule operator : Equal dnsPolicy : ClusterFirst restartPolicy : Always status : {} bee pod was scheduled on node node01 despite the taint. taint/untaint https://medium.com/kubernetes-tutorials/making-sense-of-taints-and-tolerations-in-kubernetes-446e75010f4e","title":"Taints &amp; Tolerations"},{"location":"Kubernetes/02%20Scheduling/15%20-%20Node%20Affinity/","text":"Node Affinity \u00b6 Node affinity is a set of rules. It is used by the scheduler to decide where a pod can be placed in the cluster . apply label on a node k apply label node node01 color = blue Set Node Affinity to the deployment to place the pods on node01 only. Name: blue Replicas: 3 Image: nginx NodeAffinity: requiredDuringSchedulingIgnoredDuringExecution Key: color value: blue solution: k create deployment blue2 --image=nginx --replicas=3 --dry-run=client -o yaml > deploy.yaml --- edit deploy.yaml and under spec add ... affinity: nodeAffinity: requiredDuringSchedulingIgnoredDuringExecution: nodeSelectorTerms: - matchExpressions: - key: color operator: In values: - blue ... Create a new deployment named red with the nginx image and 2 replicas, and ensure it gets placed on the controlplane node only. Use the label key - node-role.kubernetes.io/master - which is already set on the controlplane node. Name: red Replicas: 2 Image: nginx NodeAffinity: requiredDuringSchedulingIgnoredDuringExecution Key: node-role.kubernetes.io/master Use the right operator solution: apiVersion : apps/v1 kind : Deployment metadata : creationTimestamp : null labels : app : red name : red spec : replicas : 2 selector : matchLabels : app : red strategy : {} template : metadata : creationTimestamp : null labels : app : red spec : containers : - image : nginx name : nginx resources : {} affinity : nodeAffinity : requiredDuringSchedulingIgnoredDuringExecution : nodeSelectorTerms : - matchExpressions : - key : node-role.kubernetes.io/master operator : Exists status : {}","title":"Node Affinity"},{"location":"Kubernetes/02%20Scheduling/15%20-%20Node%20Affinity/#node-affinity","text":"Node affinity is a set of rules. It is used by the scheduler to decide where a pod can be placed in the cluster . apply label on a node k apply label node node01 color = blue Set Node Affinity to the deployment to place the pods on node01 only. Name: blue Replicas: 3 Image: nginx NodeAffinity: requiredDuringSchedulingIgnoredDuringExecution Key: color value: blue solution: k create deployment blue2 --image=nginx --replicas=3 --dry-run=client -o yaml > deploy.yaml --- edit deploy.yaml and under spec add ... affinity: nodeAffinity: requiredDuringSchedulingIgnoredDuringExecution: nodeSelectorTerms: - matchExpressions: - key: color operator: In values: - blue ... Create a new deployment named red with the nginx image and 2 replicas, and ensure it gets placed on the controlplane node only. Use the label key - node-role.kubernetes.io/master - which is already set on the controlplane node. Name: red Replicas: 2 Image: nginx NodeAffinity: requiredDuringSchedulingIgnoredDuringExecution Key: node-role.kubernetes.io/master Use the right operator solution: apiVersion : apps/v1 kind : Deployment metadata : creationTimestamp : null labels : app : red name : red spec : replicas : 2 selector : matchLabels : app : red strategy : {} template : metadata : creationTimestamp : null labels : app : red spec : containers : - image : nginx name : nginx resources : {} affinity : nodeAffinity : requiredDuringSchedulingIgnoredDuringExecution : nodeSelectorTerms : - matchExpressions : - key : node-role.kubernetes.io/master operator : Exists status : {}","title":"Node Affinity"},{"location":"Kubernetes/02%20Scheduling/20%20-%20Resource%20limits/","text":"Resource Limits \u00b6 We have a pod which requires 1 CPU and the pod is crushing. Node specs: The node doesnt meet the requirements for the pod. The status OOMKilled indicates that it is failing because the pod ran out of memory. Identify the memory limit set on the POD. Container resources example --- apiVersion : v1 kind : Pod metadata : name : frontend spec : containers : - name : app image : images.my-company.example/app:v4 resources : requests : memory : \"64Mi\" cpu : \"250m\" limits : memory : \"128Mi\" cpu : \"500m\" - name : log-aggregator image : images.my-company.example/log-aggregator:v6 resources : requests : memory : \"64Mi\" cpu : \"250m\" limits : memory : \"128Mi\" cpu : \"500m\"","title":"Resource Limits"},{"location":"Kubernetes/02%20Scheduling/20%20-%20Resource%20limits/#resource-limits","text":"We have a pod which requires 1 CPU and the pod is crushing. Node specs: The node doesnt meet the requirements for the pod. The status OOMKilled indicates that it is failing because the pod ran out of memory. Identify the memory limit set on the POD. Container resources example --- apiVersion : v1 kind : Pod metadata : name : frontend spec : containers : - name : app image : images.my-company.example/app:v4 resources : requests : memory : \"64Mi\" cpu : \"250m\" limits : memory : \"128Mi\" cpu : \"500m\" - name : log-aggregator image : images.my-company.example/log-aggregator:v6 resources : requests : memory : \"64Mi\" cpu : \"250m\" limits : memory : \"128Mi\" cpu : \"500m\"","title":"Resource Limits"},{"location":"Kubernetes/02%20Scheduling/25%20-%20DaemonSets/","text":"DaemonSets \u00b6 DaemonSet ensures that all (or some, matching a node selector) Nodes run a copy of a Pod. As nodes are added to the cluster, Pods are added to them. As nodes are removed from the cluster, those Pods are garbage collected. Deleting a DaemonSet will clean up the Pods it created. DaemonSet implements a single instance of a pod on all (or filtered subset of) worker node(s). Some typical uses of a DaemonSet are: Running a cluster storage daemon on every node. Running a logs collection daemon on every node. Running a node monitoring daemon on every node. Task: Deploy a DaemonSet for FluentD Logging. Use the given specifications. Name: elasticsearch Namespace: kube-system Image: k8s.gcr.io/fluentd-elasticsearch:1.20 Solution: An easy way to create a DaemonSet is to first generate a YAML file for a Deployment with the command kubectl create deployment elasticsearch --image=k8s.gcr.io/fluentd-elasticsearch:1.20 -n kube-system --dry-run=client -o yaml > fluentd.yaml . Next, remove the replicas, strategy and status fields from the YAML file using a text editor. Also, change the kind from Deployment to DaemonSet . Finally, create the Daemonset by running: kubectl apply -f fluentd.yaml apiVersion : apps/v1 kind : DaemonSet metadata : creationTimestamp : null labels : app : elasticsearch name : elasticsearch namespace : kube-system spec : selector : matchLabels : app : elasticsearch template : metadata : creationTimestamp : null labels : app : elasticsearch spec : containers : - image : k8s.gcr.io/fluentd-elasticsearch:1.20 name : fluentd-elasticsearch resources : {}","title":"DaemonSets"},{"location":"Kubernetes/02%20Scheduling/25%20-%20DaemonSets/#daemonsets","text":"DaemonSet ensures that all (or some, matching a node selector) Nodes run a copy of a Pod. As nodes are added to the cluster, Pods are added to them. As nodes are removed from the cluster, those Pods are garbage collected. Deleting a DaemonSet will clean up the Pods it created. DaemonSet implements a single instance of a pod on all (or filtered subset of) worker node(s). Some typical uses of a DaemonSet are: Running a cluster storage daemon on every node. Running a logs collection daemon on every node. Running a node monitoring daemon on every node. Task: Deploy a DaemonSet for FluentD Logging. Use the given specifications. Name: elasticsearch Namespace: kube-system Image: k8s.gcr.io/fluentd-elasticsearch:1.20 Solution: An easy way to create a DaemonSet is to first generate a YAML file for a Deployment with the command kubectl create deployment elasticsearch --image=k8s.gcr.io/fluentd-elasticsearch:1.20 -n kube-system --dry-run=client -o yaml > fluentd.yaml . Next, remove the replicas, strategy and status fields from the YAML file using a text editor. Also, change the kind from Deployment to DaemonSet . Finally, create the Daemonset by running: kubectl apply -f fluentd.yaml apiVersion : apps/v1 kind : DaemonSet metadata : creationTimestamp : null labels : app : elasticsearch name : elasticsearch namespace : kube-system spec : selector : matchLabels : app : elasticsearch template : metadata : creationTimestamp : null labels : app : elasticsearch spec : containers : - image : k8s.gcr.io/fluentd-elasticsearch:1.20 name : fluentd-elasticsearch resources : {}","title":"DaemonSets"},{"location":"Kubernetes/02%20Scheduling/30%20-%20Static%20Pods/","text":"Static PODs \u00b6 managed directly by kubelet daemon on a specific node static Pods running on a node are visible on the API server, but cannot be controlled from there. Unlike Pods that are managed by the control plane (for example, a Deployment ); instead, the kubelet watches each static Pod (and restarts it if it fails). The name of static pod is suffixed with the name of the node The spec of a static Pod cannot refer to other API objects (e.g., ServiceAccount , ConfigMap , Secret , etc). The static pod definition files path can be found in the config files of the kubelet daemon on a specific node. # -e shows all the current processes, -f shows full format listing, search for --config, which shows the config files for the kubelet ps -ef | grep kubelet # result looks something like this --config=/var/lib/kubelet/config.yaml # search for static inside the config file grep -i static /var/lib/kubelet/config.yaml # result is like this staticPodPath: /etc/kubernetes/manifests # Run this command on the node where the kubelet is running systemctl restart kubelet this is an example of static pods running in a node Remember? The name of static pod is suffixed with the name of the node. How to create a static pod? Move it to /etc/kubernetes/manifests","title":"Static PODs"},{"location":"Kubernetes/02%20Scheduling/30%20-%20Static%20Pods/#static-pods","text":"managed directly by kubelet daemon on a specific node static Pods running on a node are visible on the API server, but cannot be controlled from there. Unlike Pods that are managed by the control plane (for example, a Deployment ); instead, the kubelet watches each static Pod (and restarts it if it fails). The name of static pod is suffixed with the name of the node The spec of a static Pod cannot refer to other API objects (e.g., ServiceAccount , ConfigMap , Secret , etc). The static pod definition files path can be found in the config files of the kubelet daemon on a specific node. # -e shows all the current processes, -f shows full format listing, search for --config, which shows the config files for the kubelet ps -ef | grep kubelet # result looks something like this --config=/var/lib/kubelet/config.yaml # search for static inside the config file grep -i static /var/lib/kubelet/config.yaml # result is like this staticPodPath: /etc/kubernetes/manifests # Run this command on the node where the kubelet is running systemctl restart kubelet this is an example of static pods running in a node Remember? The name of static pod is suffixed with the name of the node. How to create a static pod? Move it to /etc/kubernetes/manifests","title":"Static PODs"},{"location":"Kubernetes/02%20Scheduling/35%20-%20Multiple%20Schedulers/","text":"Multiple Schedulers \u00b6 we need a ServiceAccount and ClusterRoleBinding that our custom scheduler will make use of. kubectl get serviceaccount -n kube-system & kubectl get clusterrolebinding ConfigMap as volume \u00b6 my-scheduler-config.yaml apiVersion : kubescheduler.config.k8s.io/v1beta2 kind : KubeSchedulerConfiguration profiles : - schedulerName : my-scheduler leaderElection : leaderElect : false k create configmap my-scheduler-config --from-file = my-scheduler-config.yaml -n kube-system my-scheduler.yaml apiVersion : v1 kind : Pod metadata : labels : run : my-scheduler name : my-scheduler namespace : kube-system spec : serviceAccountName : my-scheduler containers : - command : - /usr/local/bin/kube-scheduler - --config=/etc/kubernetes/my-scheduler/my-scheduler-config.yaml image : k8s.gcr.io/kube-scheduler:v1.23.0 livenessProbe : httpGet : path : /healthz port : 10259 scheme : HTTPS initialDelaySeconds : 15 name : kube-second-scheduler readinessProbe : httpGet : path : /healthz port : 10259 scheme : HTTPS resources : requests : cpu : '0.1' securityContext : privileged : false volumeMounts : - name : config-volume mountPath : /etc/kubernetes/my-scheduler hostNetwork : false hostPID : false volumes : - name : config-volume configMap : name : my-scheduler-config creating a POD with the new custom scheduler apiVersion : v1 kind : Pod metadata : name : nginx spec : schedulerName : my-scheduler containers : - image : nginx name : nginx","title":"Multiple Schedulers"},{"location":"Kubernetes/02%20Scheduling/35%20-%20Multiple%20Schedulers/#multiple-schedulers","text":"we need a ServiceAccount and ClusterRoleBinding that our custom scheduler will make use of. kubectl get serviceaccount -n kube-system & kubectl get clusterrolebinding","title":"Multiple Schedulers"},{"location":"Kubernetes/02%20Scheduling/35%20-%20Multiple%20Schedulers/#configmap-as-volume","text":"my-scheduler-config.yaml apiVersion : kubescheduler.config.k8s.io/v1beta2 kind : KubeSchedulerConfiguration profiles : - schedulerName : my-scheduler leaderElection : leaderElect : false k create configmap my-scheduler-config --from-file = my-scheduler-config.yaml -n kube-system my-scheduler.yaml apiVersion : v1 kind : Pod metadata : labels : run : my-scheduler name : my-scheduler namespace : kube-system spec : serviceAccountName : my-scheduler containers : - command : - /usr/local/bin/kube-scheduler - --config=/etc/kubernetes/my-scheduler/my-scheduler-config.yaml image : k8s.gcr.io/kube-scheduler:v1.23.0 livenessProbe : httpGet : path : /healthz port : 10259 scheme : HTTPS initialDelaySeconds : 15 name : kube-second-scheduler readinessProbe : httpGet : path : /healthz port : 10259 scheme : HTTPS resources : requests : cpu : '0.1' securityContext : privileged : false volumeMounts : - name : config-volume mountPath : /etc/kubernetes/my-scheduler hostNetwork : false hostPID : false volumes : - name : config-volume configMap : name : my-scheduler-config creating a POD with the new custom scheduler apiVersion : v1 kind : Pod metadata : name : nginx spec : schedulerName : my-scheduler containers : - image : nginx name : nginx","title":"ConfigMap as volume"},{"location":"Kubernetes/03%20Logging%20%26%20Monitoring/00%20-%20Monitor%20Cluster%20Components/","text":"Monitor Cluster components \u00b6 deploying metrics server to monitor PODs & Nodes kubectl apply -f https://github.com/kubernetes-sigs/metrics-server/releases/latest/download/components.yaml You might face with this problem: fix: download the components.yaml and add --kubelet-insecure-tls template : metadata : labels : k8s-app : metrics-server spec : containers : - args : - --cert-dir=/tmp - --secure-port=4443 - --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname - --kubelet-use-node-status-port - --metric-resolution=15s - --kubelet-insecure-tls image : k8s.gcr.io/metrics-server/metrics-server:v0.5.2 Usage of --kubelet-insecure-tls is expected in clusters that don't provide proper certificates to Kubelet. Such clusters should not be used in production The top command allows you to see the resource consumption for nodes or pods.","title":"Monitor Cluster components"},{"location":"Kubernetes/03%20Logging%20%26%20Monitoring/00%20-%20Monitor%20Cluster%20Components/#monitor-cluster-components","text":"deploying metrics server to monitor PODs & Nodes kubectl apply -f https://github.com/kubernetes-sigs/metrics-server/releases/latest/download/components.yaml You might face with this problem: fix: download the components.yaml and add --kubelet-insecure-tls template : metadata : labels : k8s-app : metrics-server spec : containers : - args : - --cert-dir=/tmp - --secure-port=4443 - --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname - --kubelet-use-node-status-port - --metric-resolution=15s - --kubelet-insecure-tls image : k8s.gcr.io/metrics-server/metrics-server:v0.5.2 Usage of --kubelet-insecure-tls is expected in clusters that don't provide proper certificates to Kubelet. Such clusters should not be used in production The top command allows you to see the resource consumption for nodes or pods.","title":"Monitor Cluster components"},{"location":"Kubernetes/03%20Logging%20%26%20Monitoring/05%20-%20Managing%20Application%20logs/","text":"Managing Application logs \u00b6 Application logs can help you understand what is happening inside your application. k logs Print the logs for a container in a pod or specified resource. If the pod has only one container, the container name is optional.","title":"Managing Application logs"},{"location":"Kubernetes/03%20Logging%20%26%20Monitoring/05%20-%20Managing%20Application%20logs/#managing-application-logs","text":"Application logs can help you understand what is happening inside your application. k logs Print the logs for a container in a pod or specified resource. If the pod has only one container, the container name is optional.","title":"Managing Application logs"},{"location":"Kubernetes/04%20Application%20Lifecycle%20Management/00%20-%20Test%20rolling%20updates%20%26%20rollbacks/","text":"Rolling updates & rollbacks \u00b6 The rolling update strategy is a gradual process that allows you to update your Kubernetes system with only a minor effect on performance and no downtime . In this strategy, the Deployment selects a Pod with the old programming, deactivates it, and creates an updated Pod to replace it. Max Unavailable value under RollingUpdateStrategy in deployment details shows us up to how many PODs can be down for upgrade at a time. 25% of 4 = 1 ... spec : minReadySeconds : 20 progressDeadlineSeconds : 600 replicas : 4 revisionHistoryLimit : 10 selector : matchLabels : name : webapp strategy : rollingUpdate : maxSurge : 25% maxUnavailable : 25% type : RollingUpdate ---","title":"Rolling updates & rollbacks"},{"location":"Kubernetes/04%20Application%20Lifecycle%20Management/00%20-%20Test%20rolling%20updates%20%26%20rollbacks/#rolling-updates-rollbacks","text":"The rolling update strategy is a gradual process that allows you to update your Kubernetes system with only a minor effect on performance and no downtime . In this strategy, the Deployment selects a Pod with the old programming, deactivates it, and creates an updated Pod to replace it. Max Unavailable value under RollingUpdateStrategy in deployment details shows us up to how many PODs can be down for upgrade at a time. 25% of 4 = 1 ... spec : minReadySeconds : 20 progressDeadlineSeconds : 600 replicas : 4 revisionHistoryLimit : 10 selector : matchLabels : name : webapp strategy : rollingUpdate : maxSurge : 25% maxUnavailable : 25% type : RollingUpdate ---","title":"Rolling updates &amp; rollbacks"},{"location":"Kubernetes/04%20Application%20Lifecycle%20Management/05%20-%20Commands%20%26%20Arguments/","text":"Commands & Arguments \u00b6 commands in k8s manifest ... spec : containers : - name : ubuntu image : ubuntu command : - \"sleep\" - \"5000\" ... commands in Dockerfile ENTRYPOINT [\"python\", \"app.py\"] The ENTRYPOINT in the Dockerfile is overridden by the command in the pod definition. FROM python:3.6-alpine RUN pip install flask COPY . /opt/ EXPOSE 8080 WORKDIR /opt ENTRYPOINT [\"python\", \"app.py\"] CMD [\"--color\", \"red\"] vs. apiVersion : v1 kind : Pod metadata : name : webapp-green labels : name : webapp-green spec : containers : - name : simple-webapp image : kodekloud/webapp-color command : [ \"--color\" , \"green\" ] Since the entrypoint is overridden in the pod definition, the command that will be run is just --color green arguments ... spec : containers : - name : simple-webapp image : kodekloud/webapp-color command : [ \"python\" , \"app.py\" ] args : [ \"--color\" , \"pink\" ] ... python app.py --color pink will be executed. creating a pod with args apiVersion : v1 kind : Pod metadata : name : webapp-green labels : name : webapp-green spec : containers : - name : simple-webapp image : kodekloud/webapp-color args : [ \"--color\" , \"green\" ]","title":"Commands & Arguments"},{"location":"Kubernetes/04%20Application%20Lifecycle%20Management/05%20-%20Commands%20%26%20Arguments/#commands-arguments","text":"commands in k8s manifest ... spec : containers : - name : ubuntu image : ubuntu command : - \"sleep\" - \"5000\" ... commands in Dockerfile ENTRYPOINT [\"python\", \"app.py\"] The ENTRYPOINT in the Dockerfile is overridden by the command in the pod definition. FROM python:3.6-alpine RUN pip install flask COPY . /opt/ EXPOSE 8080 WORKDIR /opt ENTRYPOINT [\"python\", \"app.py\"] CMD [\"--color\", \"red\"] vs. apiVersion : v1 kind : Pod metadata : name : webapp-green labels : name : webapp-green spec : containers : - name : simple-webapp image : kodekloud/webapp-color command : [ \"--color\" , \"green\" ] Since the entrypoint is overridden in the pod definition, the command that will be run is just --color green arguments ... spec : containers : - name : simple-webapp image : kodekloud/webapp-color command : [ \"python\" , \"app.py\" ] args : [ \"--color\" , \"pink\" ] ... python app.py --color pink will be executed. creating a pod with args apiVersion : v1 kind : Pod metadata : name : webapp-green labels : name : webapp-green spec : containers : - name : simple-webapp image : kodekloud/webapp-color args : [ \"--color\" , \"green\" ]","title":"Commands &amp; Arguments"},{"location":"Kubernetes/04%20Application%20Lifecycle%20Management/10%20-%20Env%20Variables/","text":"Env Variables \u00b6 creating a pod with with Label name webappcolor & env APP_COLOR=green k run webapp-color --image = kodekloud/webapp-color --labels = \"name=webapp-color\" --env = \"APP_COLOR=green\" creating a ConfigMap for a pod with given data: APP_COLOR=darkblue k create configmap webapp-config-map --from-literal = APP_COLOR = darkblue updating env variable on a POD to use newly created ConfigMap ... spec : containers : - envFrom : - configMapRef : name : webapp-config-map ...","title":"Env Variables"},{"location":"Kubernetes/04%20Application%20Lifecycle%20Management/10%20-%20Env%20Variables/#env-variables","text":"creating a pod with with Label name webappcolor & env APP_COLOR=green k run webapp-color --image = kodekloud/webapp-color --labels = \"name=webapp-color\" --env = \"APP_COLOR=green\" creating a ConfigMap for a pod with given data: APP_COLOR=darkblue k create configmap webapp-config-map --from-literal = APP_COLOR = darkblue updating env variable on a POD to use newly created ConfigMap ... spec : containers : - envFrom : - configMapRef : name : webapp-config-map ...","title":"Env Variables"},{"location":"Kubernetes/04%20Application%20Lifecycle%20Management/15%20-%20Secrets/","text":"Secrets \u00b6","title":"Secrets"},{"location":"Kubernetes/04%20Application%20Lifecycle%20Management/15%20-%20Secrets/#secrets","text":"","title":"Secrets"},{"location":"Kubernetes/04%20Application%20Lifecycle%20Management/Untitled%204/","text":"","title":"Untitled 4"},{"location":"Kubernetes/04%20Application%20Lifecycle%20Management/Untitled%205/","text":"","title":"Untitled 5"},{"location":"Kubernetes/05%20Cluster%20Maintenance/OS%20Upgrades/","text":"OS Upgrades \u00b6 We have a cluster with 2 nodes. To check how many applications are hosted on the cluster: Which nodes are the applications hosted on? We need to take node01 out for maintenance. Empty the node of all applications and mark it unschedulable. What nodes are the apps on now? Applying Patches... The maintenance tasks have been completed. Configure the node node01 to be schedulable again. However, there are no pods scheduled on node01 . Only when new pods are created they will be scheduled. The pods are placed on the controlplane node, because it does not have any taints. If there is a pod which is not part of a replicaset, we cannot drain node. A forceful drain of the node will delete any pod that is not part of a replicaset. To mark a node as unschedulable so that no new pods are scheduled on this node we can: Takeaways \u00b6 Taints and Tolerations \u2013 Concepts Taints and tolerations are a mechanism that allows you to ensure that pods are not placed on inappropriate nodes. Taints are added to nodes, while tolerations are defined in the pod specification. When you taint a node, it will repel all the pods except those that have a toleration for that taint. A node can have one or many taints associated with it. For example, most Kubernetes distributions will automatically taint the master nodes so that one of the pods that manages the control plane is scheduled onto them and not any other data plane pods deployed by users. This ensures that the master nodes are dedicated to run control plane pods. A taint can produce three possible effects: NoSchedule The Kubernetes scheduler will only allow scheduling pods that have tolerations for the tainted nodes. PreferNoSchedule The Kubernetes scheduler will try to avoid scheduling pods that don\u2019t have tolerations for the tainted nodes. NoExecute Kubernetes will evict the running pods from the nodes if the pods don\u2019t have tolerations for the tainted nodes. Commands used: kubectl cordon my-node # Mark my-node as unschedulable kubectl drain my-node # Drain my-node in preparation for maintenance kubectl uncordon my-node # Mark my-node as schedulable","title":"OS Upgrades"},{"location":"Kubernetes/05%20Cluster%20Maintenance/OS%20Upgrades/#os-upgrades","text":"We have a cluster with 2 nodes. To check how many applications are hosted on the cluster: Which nodes are the applications hosted on? We need to take node01 out for maintenance. Empty the node of all applications and mark it unschedulable. What nodes are the apps on now? Applying Patches... The maintenance tasks have been completed. Configure the node node01 to be schedulable again. However, there are no pods scheduled on node01 . Only when new pods are created they will be scheduled. The pods are placed on the controlplane node, because it does not have any taints. If there is a pod which is not part of a replicaset, we cannot drain node. A forceful drain of the node will delete any pod that is not part of a replicaset. To mark a node as unschedulable so that no new pods are scheduled on this node we can:","title":"OS Upgrades"},{"location":"Kubernetes/05%20Cluster%20Maintenance/OS%20Upgrades/#takeaways","text":"Taints and Tolerations \u2013 Concepts Taints and tolerations are a mechanism that allows you to ensure that pods are not placed on inappropriate nodes. Taints are added to nodes, while tolerations are defined in the pod specification. When you taint a node, it will repel all the pods except those that have a toleration for that taint. A node can have one or many taints associated with it. For example, most Kubernetes distributions will automatically taint the master nodes so that one of the pods that manages the control plane is scheduled onto them and not any other data plane pods deployed by users. This ensures that the master nodes are dedicated to run control plane pods. A taint can produce three possible effects: NoSchedule The Kubernetes scheduler will only allow scheduling pods that have tolerations for the tainted nodes. PreferNoSchedule The Kubernetes scheduler will try to avoid scheduling pods that don\u2019t have tolerations for the tainted nodes. NoExecute Kubernetes will evict the running pods from the nodes if the pods don\u2019t have tolerations for the tainted nodes. Commands used: kubectl cordon my-node # Mark my-node as unschedulable kubectl drain my-node # Drain my-node in preparation for maintenance kubectl uncordon my-node # Mark my-node as schedulable","title":"Takeaways"},{"location":"PNPT/Week%200%20%28Start%20Here%29/","text":"Setting up.. \u00b6 Course overview A Day in the Life of an Ethical Hacker Installing VMware/VirtualBox Configuring network Effective Notekeeping (SO IMPORTANT!) GreenShot: https://getgreenshot.org/downloads/ FlameShot: https://github.com/lupoDharkael/flameshot","title":"Setting up.."},{"location":"PNPT/Week%200%20%28Start%20Here%29/#setting-up","text":"Course overview A Day in the Life of an Ethical Hacker Installing VMware/VirtualBox Configuring network Effective Notekeeping (SO IMPORTANT!) GreenShot: https://getgreenshot.org/downloads/ FlameShot: https://github.com/lupoDharkael/flameshot","title":"Setting up.."},{"location":"PNPT/Week%201%20%286-22%20%26%206-24%29%20-%20Linux%20%26%20Networking/","text":"Linux & Networking \u00b6 Linux \u00b6 The Hacker Training Path: https://tcm-sec.com/so-you-want-to-be-a-hacker-2022-edition/ Explain Shell (for Linux): https://explainshell.com Linux for Ethical Hackers (free on YouTube): https://youtu.be/U1w4T03B30I Networking \u00b6 Seven Second Subnetting: https://www.youtube.com/watch?v=ZxAwQB8TZsM Subnet Guide: https://drive.google.com/file/d/1ETKH31-E7G-7ntEOlWGZcDZWuukmeHFe/view Installing and updating Tools \u00b6 sudo apt update && apt upgrade Tip: You might want to PimpYourKali Ping Sweep Script \u00b6 #!/bin/bash if [ \" $1 \" == \"\" ] then echo \"You forgot an IP address!\" echo \"Syntax: ./ipsweep.sh 192.168.5\" else for ip in ` seq 1 254 ` ; do ping -c 1 $1 . $ip | grep \"64 bytes\" | cut -d \" \" -f 4 & done fi","title":"Linux & Networking"},{"location":"PNPT/Week%201%20%286-22%20%26%206-24%29%20-%20Linux%20%26%20Networking/#linux-networking","text":"","title":"Linux &amp; Networking"},{"location":"PNPT/Week%201%20%286-22%20%26%206-24%29%20-%20Linux%20%26%20Networking/#linux","text":"The Hacker Training Path: https://tcm-sec.com/so-you-want-to-be-a-hacker-2022-edition/ Explain Shell (for Linux): https://explainshell.com Linux for Ethical Hackers (free on YouTube): https://youtu.be/U1w4T03B30I","title":"Linux"},{"location":"PNPT/Week%201%20%286-22%20%26%206-24%29%20-%20Linux%20%26%20Networking/#networking","text":"Seven Second Subnetting: https://www.youtube.com/watch?v=ZxAwQB8TZsM Subnet Guide: https://drive.google.com/file/d/1ETKH31-E7G-7ntEOlWGZcDZWuukmeHFe/view","title":"Networking"},{"location":"PNPT/Week%201%20%286-22%20%26%206-24%29%20-%20Linux%20%26%20Networking/#installing-and-updating-tools","text":"sudo apt update && apt upgrade Tip: You might want to PimpYourKali","title":"Installing and updating Tools"},{"location":"PNPT/Week%201%20%286-22%20%26%206-24%29%20-%20Linux%20%26%20Networking/#ping-sweep-script","text":"#!/bin/bash if [ \" $1 \" == \"\" ] then echo \"You forgot an IP address!\" echo \"Syntax: ./ipsweep.sh 192.168.5\" else for ip in ` seq 1 254 ` ; do ping -c 1 $1 . $ip | grep \"64 bytes\" | cut -d \" \" -f 4 & done fi","title":"Ping Sweep Script"}]}